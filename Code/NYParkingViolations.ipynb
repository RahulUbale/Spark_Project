{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Anaylsis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: string (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Plate Type: string (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Violation Code: string (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Issuing Agency: string (nullable = true)\n",
      " |-- Street Code1: string (nullable = true)\n",
      " |-- Street Code2: string (nullable = true)\n",
      " |-- Street Code3: string (nullable = true)\n",
      " |-- Vehicle Expiration Date: string (nullable = true)\n",
      " |-- Violation Location: string (nullable = true)\n",
      " |-- Violation Precinct: string (nullable = true)\n",
      " |-- Issuer Precinct: string (nullable = true)\n",
      " |-- Issuer Code: string (nullable = true)\n",
      " |-- Issuer Command: string (nullable = true)\n",
      " |-- Issuer Squad: string (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      " |-- Time First Observed: string (nullable = true)\n",
      " |-- Violation County: string (nullable = true)\n",
      " |-- Violation In Front Of Or Opposite: string (nullable = true)\n",
      " |-- House Number: string (nullable = true)\n",
      " |-- Street Name: string (nullable = true)\n",
      " |-- Intersecting Street: string (nullable = true)\n",
      " |-- Date First Observed: string (nullable = true)\n",
      " |-- Law Section: string (nullable = true)\n",
      " |-- Sub Division: string (nullable = true)\n",
      " |-- Violation Legal Code: string (nullable = true)\n",
      " |-- Days Parking In Effect    : string (nullable = true)\n",
      " |-- From Hours In Effect: string (nullable = true)\n",
      " |-- To Hours In Effect: string (nullable = true)\n",
      " |-- Vehicle Color: string (nullable = true)\n",
      " |-- Unregistered Vehicle?: string (nullable = true)\n",
      " |-- Vehicle Year: string (nullable = true)\n",
      " |-- Meter Number: string (nullable = true)\n",
      " |-- Feet From Curb: string (nullable = true)\n",
      " |-- Violation Post Code: string (nullable = true)\n",
      " |-- Violation Description: string (nullable = true)\n",
      " |-- No Standing or Stopping Violation: string (nullable = true)\n",
      " |-- Hydrant Violation: string (nullable = true)\n",
      " |-- Double Parking Violation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TicketCounts\").getOrCreate()\n",
    "# Read the input data from CSV file\n",
    "df = spark.read.csv(\"D:/Users/Rahul/Parking_Violations_Issued_-_Fiscal_Year_2023.csv\", header=True)\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "#\"C:/Users/Lenovo/Desktop/shot_logs.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Problem1\n",
    "When are tickets most likely to be issued? (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|Violation Time|NumViolations|\n",
      "+--------------+-------------+\n",
      "|         0836A|        22958|\n",
      "|         0839A|        21772|\n",
      "|         0906A|        21712|\n",
      "|         0840A|        21588|\n",
      "|         0838A|        21585|\n",
      "|         0837A|        21074|\n",
      "|         0841A|        20883|\n",
      "|         0908A|        20765|\n",
      "|         0842A|        20660|\n",
      "|         1141A|        20564|\n",
      "|         1140A|        20502|\n",
      "|         1142A|        20497|\n",
      "|         0909A|        20488|\n",
      "|         0843A|        20473|\n",
      "|         0910A|        20421|\n",
      "|         0907A|        20249|\n",
      "|         1138A|        20171|\n",
      "|         1139A|        20156|\n",
      "|         0844A|        20152|\n",
      "|         0845A|        20089|\n",
      "+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By thw analysis of Violation Time\n",
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"D:/Users/Rahul/Parking_Violations_Issued_-_Fiscal_Year_2023.csv\", header=True)\n",
    "\n",
    "# group by violation time and count number of violations\n",
    "grouped_df = df.groupBy(\"Violation Time\").agg(count(\"*\").alias(\"NumViolations\"))\n",
    "\n",
    "# sort the result in descending order of the number of violations\n",
    "result_df = grouped_df.orderBy(col(\"NumViolations\").desc())\n",
    "\n",
    "# display the result\n",
    "result_df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data given, we can conclude that tickets are most likely to be issued at 8:36 AM (0836A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|Issue Date|Num Tickets Issued|\n",
      "+----------+------------------+\n",
      "|2022-08-04|             66726|\n",
      "|2022-08-05|             65393|\n",
      "|2022-08-02|             64876|\n",
      "|2022-06-30|             64846|\n",
      "|2022-07-19|             64815|\n",
      "|2022-11-25|             64411|\n",
      "|2022-08-11|             64192|\n",
      "|2022-08-18|             63975|\n",
      "|2022-07-12|             63780|\n",
      "|2022-07-15|             63646|\n",
      "+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By the anaylsis of Issue Date \n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"D:/Users/Rahul/Parking_Violations_Issued_-_Fiscal_Year_2023.csv\", header=True)\n",
    "\n",
    "# Convert the \"Issue Date\" column to a date type\n",
    "df = df.withColumn(\"Issue Date\", to_date(\"Issue Date\", \"MM/dd/yyyy\"))\n",
    "\n",
    "# Group the data by \"Issue Date\" and count the number of tickets issued on each date\n",
    "df_grouped = df.groupBy(\"Issue Date\").agg(count(\"*\").alias(\"Num Tickets Issued\"))\n",
    "\n",
    "# Sort the results in descending order by the number of tickets issued\n",
    "df_sorted = df_grouped.orderBy(\"Num Tickets Issued\", ascending=False)\n",
    "\n",
    "# Show all rows of the sorted DataFrame\n",
    "df_sorted.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can conclude that August 4, 2022 (Aug Month), is the day when tickets are most likely to be issued"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# Problem 2\n",
    "What are the most common years and types of cars to be ticketed? (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+\n",
      "|Vehicle Year|Vehicle Make|Num Tickets|\n",
      "+------------+------------+-----------+\n",
      "|           0|       HONDA|     208547|\n",
      "|           0|        FORD|     168583|\n",
      "|           0|       TOYOT|     165496|\n",
      "|           0|       NISSA|     139312|\n",
      "|        2021|       TOYOT|     117999|\n",
      "|        2019|       HONDA|     113890|\n",
      "|        2021|       HONDA|     107202|\n",
      "|        2020|       HONDA|     104349|\n",
      "|           0|       CHEVR|      99590|\n",
      "|           0|         BMW|      95905|\n",
      "+------------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"AppName\").getOrCreate()\n",
    "from pyspark.sql.functions import desc, count\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"D:/Users/Rahul/Parking_Violations_Issued_-_Fiscal_Year_2023.csv\", header=True)\n",
    "\n",
    "# Group the data by \"Vehicle Year\" and \"Vehicle Make\", and count the number of tickets for each group\n",
    "df_grouped = df.groupBy(\"Vehicle Year\", \"Vehicle Make\").agg(count(\"*\").alias(\"Num Tickets\"))\n",
    "\n",
    "# Sort the results in descending order by the number of tickets\n",
    "df_sorted = df_grouped.sort(desc(\"Num Tickets\"))\n",
    "\n",
    "# Show the top 10 rows of the sorted DataFrame\n",
    "df_sorted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+\n",
      "|Vehicle Year|Vehicle Make|Num Tickets|\n",
      "+------------+------------+-----------+\n",
      "|        2021|       TOYOT|     117999|\n",
      "|        2019|       HONDA|     113890|\n",
      "|        2021|       HONDA|     107202|\n",
      "|        2020|       HONDA|     104349|\n",
      "|        2022|       TOYOT|      95782|\n",
      "|        2022|       HONDA|      93183|\n",
      "|        2018|       TOYOT|      91268|\n",
      "|        2018|       HONDA|      91213|\n",
      "|        2017|       TOYOT|      88740|\n",
      "|        2019|        FORD|      86105|\n",
      "+------------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows where Vehicle Year is 0 as these are null data \n",
    "df_filtered = df_grouped.filter(df_grouped[\"Vehicle Year\"] != \"0\")\n",
    "\n",
    "# Sort the results in descending order by the number of tickets\n",
    "df_sorted = df_filtered.sort(desc(\"Num Tickets\"))\n",
    "\n",
    "# Show the top 10 rows of the sorted DataFrame\n",
    "df_sorted.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "#See Whole data in ans folder\n",
    "#df_sorted.show(df_sorted.count())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding data we can say that TOYOTA cars in year  2021 has most number of ticket,in 2nd place it is Honda cars in 2019.In 3rd place it is again Honda cars in year 2021 who got 107202 tickets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Where are tickets most commonly issued? (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|Violation County|num_tickets|\n",
      "+----------------+-----------+\n",
      "|              NY|    2450153|\n",
      "|              QN|    1858441|\n",
      "|              BK|    1732079|\n",
      "|              BX|    1497854|\n",
      "|               K|    1365103|\n",
      "|               Q|    1215635|\n",
      "|              MN|     603098|\n",
      "|              ST|     328334|\n",
      "|           Kings|     188817|\n",
      "|               R|      99407|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, desc\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"D:/Users/Rahul/Parking_Violations_Issued_-_Fiscal_Year_2023.csv\", header=True)\n",
    "\n",
    "# Group the data by Violation County and count the number of violations\n",
    "ticket_counts = df.groupBy(\"Violation County\").agg(count(\"*\").alias(\"num_tickets\"))\n",
    "\n",
    "# Sort the results in descending order and show the top 10 counties\n",
    "top_counties = ticket_counts.orderBy(desc(\"num_tickets\")).limit(10)\n",
    "top_counties.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that tickets are most commonly issued in New York City. The most ticketed county is NY (which likely refers to the borough of Manhattan), with 2,450,153 tickets issued. The next most ticketed counties are QN (Queens) and BK (Brooklyn), with 1,858,441 issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|Violation Precinct|num_tickets|\n",
      "+------------------+-----------+\n",
      "|                 0|    5349526|\n",
      "|                19|     282466|\n",
      "|                13|     254057|\n",
      "|                 6|     224686|\n",
      "|               114|     221523|\n",
      "|                14|     190012|\n",
      "|                18|     176733|\n",
      "|                 9|     162228|\n",
      "|                 1|     152429|\n",
      "|               109|     137833|\n",
      "+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, desc\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"D:/Users/Rahul/Parking_Violations_Issued_-_Fiscal_Year_2023.csv\", header=True)\n",
    "\n",
    "# Group the data by precinct and count the number of violations\n",
    "ticket_counts = df.groupBy(\"Violation Precinct\").agg(count(\"*\").alias(\"num_tickets\"))\n",
    "\n",
    "# Sort the results in descending order and show the top 10 precincts\n",
    "top_precincts = ticket_counts.orderBy(desc(\"num_tickets\")).limit(10)\n",
    "top_precincts.show(top_precincts.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysis of Violation Precinct,The precinct numbered 0 has the highest number of tickets issued, which is 5349526. This could indicate that either there is some error in recording the precinct number, or this number is being used as a default value for tickets with unknown precincts.In NY we can say that ,The precincts numbered 19 and 13 have the second and third highest number of tickets issued respectively, with 282466 and 254057 tickets each."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "Which color of the vehicle is most likely to get a ticket? (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color most likely to get a ticket is:  GY\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, count\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"D:/Users/Rahul/Parking_Violations_Issued_-_Fiscal_Year_2023.csv\", header=True)\n",
    "\n",
    "# Group the data by vehicle color and count the number of tickets\n",
    "color_counts = df.groupBy(\"Vehicle Color\").agg(count(\"*\").alias(\"Num Tickets\"))\n",
    "\n",
    "# Sort the results in descending order and show the top color\n",
    "top_color = color_counts.sort(desc(\"Num Tickets\")).first()\n",
    "print(\"The color most likely to get a ticket is: \", top_color[\"Vehicle Color\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|Vehicle Color|NumViolations|\n",
      "+-------------+-------------+\n",
      "|           GY|      2275457|\n",
      "+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# group by vehicle color and count the number of violations for each color\n",
    "grouped_df = parking_df.groupBy(\"Vehicle Color\") \\\n",
    "                      .agg({\"Violation Code\": \"count\"}) \\\n",
    "                      .withColumnRenamed(\"count(Violation Code)\", \"NumViolations\")\n",
    "\n",
    "# sort by the number of violations in descending order and display the first row\n",
    "result_df = grouped_df.orderBy(col(\"NumViolations\").desc())\n",
    "result_df.show(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color most likely to get a ticket is:  GY with 2275457 NumViolations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "Based on a K-Means algorithm, please try to answer the following question:\n",
    "â€¢ Given a Black vehicle parking illegally at 34510, 10030, 34050 (street codes). What is\n",
    "the probability that it will get an ticket? (very rough prediction). (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"D:/Users/Rahul/Parking_Violations_Issued_-_Fiscal_Year_2023.csv\")\n",
    "black_vehicles = df.filter(df[\"Vehicle Color\"] == \"BLACK\")\n",
    "from pyspark.sql.functions import pow, sqrt\n",
    "black_vehicles = black_vehicles.withColumn(\"Distance\", sqrt(pow(black_vehicles[\"Street Code1\"]-34510, 2) +\\\n",
    "                                                             pow(black_vehicles[\"Street Code2\"]-10030, 2) + pow(black_vehicles[\"Street Code3\"]-34050, 2)))\n",
    "selected_columns = [\"Distance\", \"Violation Code\"]\n",
    "black_vehicles = black_vehicles.select(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "black_vehicles = black_vehicles.withColumn(\"Violation Code\", col(\"Violation Code\").cast(DoubleType()))\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=selected_columns, outputCol=\"features\")\n",
    "kmeans_data = assembler.transform(black_vehicles).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(k=3, seed=1)\n",
    "model = kmeans.fit(kmeans_data)\n",
    "from pyspark.ml.linalg import Vectors\n",
    "new_point = Vectors.dense([0.1, 28])\n",
    "predicted_cluster = model.predict(new_point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clusters = model.transform(kmeans_data)\n",
    "predicted_cluster_data = all_clusters.filter(all_clusters[\"prediction\"] == predicted_cluster)\n",
    "count_predicted_cluster = predicted_cluster_data.count()\n",
    "total_count = all_clusters.count()\n",
    "probability = count_predicted_cluster / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of getting a ticket for a black vehicle parking illegally at 34510, 10030, 34050 is 0.4452690210726885\n"
     ]
    }
   ],
   "source": [
    "print(\"The probability of getting a ticket for a black vehicle parking illegally at 34510, 10030, 34050 is\", probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
